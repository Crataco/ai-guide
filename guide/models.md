# Models

### What are models?

A model is your AI. Or to be more specific, its brain.

Models come in all shapes and sizes, and you will see how they differ depending on:

- How big it is, usually determined as a number in the millions or billions (which influences how much disk space it takes, how smart the AI is, and how good your computer will have to be to run it)

- What data was used to train it (which influences what it "knows" and how it should respond)

- The backend (which can leave it incompatible with certain frontends). The most widely-supported backend is [Transformers](https://github.com/huggingface/transformers), but that's a can of worms for another time.

### How many models are there?

I've counted 8 different freely-downloadable model series ever since 2018:
- OpenAI's GPT-2 (125M to 1.5B), most notably used for AI Dungeon 2 and Talk to Transformers
- EleutherAI's GPT-Neo (125M to 2.7B), GPT-J (6B), and GPT-NeoX (20B), most notably used for NovelAI and Pygmalion 6B
- 
- EleutherAI's Pythia (70M to 12B)
- Meta AI's LLaMA (7B to 65B)
