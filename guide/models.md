![image](https://user-images.githubusercontent.com/55674863/230696024-98ce9e16-f558-4402-ac43-0e7f960c118c.png)

# Models

### What are models?

_A model is your AI. Or to be more specific, its brain._

_Models come in all shapes and sizes, and you will see how they differ depending on how big it is, what data was used to train it, and what backend it uses (which most of the time is [Transformers](https://github.com/huggingface/transformers), but that's a can of worms for another time)._

### How many models are there?

_**(If you want a broader list including model evaluation results, [see here](https://docs.google.com/spreadsheets/d/1kT4or6b0Fedd-W_jMwYpb63e1ZR3aePczz3zlbJW-Y4/edit#gid=741531996) (credit goes to [u/randomfoo2 on Reddit](https://old.reddit.com/r/LocalAI/comments/12smsy9/list_of_public_foundational_models_fine_tunes/))**_

_I note 12 downloadable, usable model series, starting at 2019. These are sorted in chronological order. There may be more (like Facebook/Meta's XGLM)._

_Looking for model finetunes? [Check here](https://github.com/Crataco/ai-guide/blob/main/guide/bonus.md)._

Series | Sizes | Dataset | License | My thoughts
:--|:--:|:--:|:--:|:--:
**GPT-2** | [124M](https://huggingface.co/gpt2), [355M](https://huggingface.co/gpt2-medium), [774M](https://huggingface.co/gpt2-large), [1.5B](https://huggingface.co/gpt2-xl) | [WebText](https://github.com/openai/gpt-2/blob/master/model_card.md#datasets) | [Modified MIT License](https://github.com/openai/gpt-2/blob/master/LICENSE) | *2019 - By OpenAI. GPT-2 kickstarted modern AI text generation, and was most notably used for [the original AI Dungeon 2](https://www.reddit.com/r/rpg/comments/e7ladj/someone_made_an_ai_dungeon_master_that_you_can/). I don't recommend it today due to its small size and small context length (1024 tokens).*
**GPT-Neo** | [125M](https://huggingface.co/EleutherAI/gpt-neo-125M), [350M](https://huggingface.co/xhyi/PT_GPTNEO350_ATG), [1.3B](https://huggingface.co/EleutherAI/gpt-neo-1.3B), [2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B), [6B](https://huggingface.co/EleutherAI/gpt-j-6b), [20B](https://huggingface.co/EleutherAI/gpt-neox-20b) | [The Pile](https://pile.eleuther.ai/) | [MIT](https://huggingface.co/EleutherAI/gpt-neo-2.7B) (125M to 2.7B), [Apache-2.0](https://huggingface.co/EleutherAI/gpt-j-6b) (6B to 20B) | *2021 - By EleutherAI's. I feel it has plenty community support.*
**Fairseq** | [125M](https://huggingface.co/KoboldAI/fairseq-dense-125M), [355M](https://huggingface.co/KoboldAI/fairseq-dense-355M), [1.3B](https://huggingface.co/KoboldAI/fairseq-dense-1.3B), [2.7B](https://huggingface.co/KoboldAI/fairseq-dense-2.7B), [6.7B](https://huggingface.co/KoboldAI/fairseq-dense-6.7B), [13B](https://huggingface.co/KoboldAI/fairseq-dense-13B) | [Training Data](https://github.com/facebookresearch/fairseq/blob/main/examples/moe_lm/model_card.md#training-data) | Likely [MIT](https://github.com/facebookresearch/fairseq/blob/main/LICENSE) | *2022 - By Meta AI, but ported to Transformers by the KoboldAI community. This picked up steam as an alternative to GPT-Neo, and was included alongside Neo with the NovelAI service. Being used in a commercial service makes me believe it uses a commercial license, so MIT is likely.*
**OPT** | [125M](https://huggingface.co/facebook/opt-125m), [350M](https://huggingface.co/facebook/opt-350m), [1.3B](https://huggingface.co/facebook/opt-1.3b), [2.7B](https://huggingface.co/facebook/opt-2.7b), [6.7B](https://huggingface.co/facebook/opt-6.7b), [13B](https://huggingface.co/facebook/opt-13b), [30B](https://huggingface.co/facebook/opt-30b), [66B](https://huggingface.co/facebook/opt-66b), 175B | [Training Data](https://huggingface.co/facebook/opt-1.3b#training-data) | [Non-commercial OPT-175B license](https://huggingface.co/facebook/opt-1.3b/blob/main/LICENSE.md) | *2022 - By Meta AI, succeeding Fairseq, their previous series. It slightly outperforms Neo. It comes second to Neo in terms of the sheer quantity of finetunes and community experience. There's a 175B, but it's only available via request. I can't find a download anywhere.*
**RWKV** | [169M](https://huggingface.co/BlinkDL/rwkv-4-pile-169m), [430M](https://huggingface.co/BlinkDL/rwkv-4-pile-430m), [1.5B](https://huggingface.co/BlinkDL/rwkv-4-pile-1b5), [3B](https://huggingface.co/BlinkDL/rwkv-4-pile-3b), [7B](https://huggingface.co/BlinkDL/rwkv-4-pile-7b), [14B](https://huggingface.co/BlinkDL/rwkv-4-pile-14b) | The Pile | [Apache-2.0](https://huggingface.co/BlinkDL/rwkv-4-pile-14b) | *2022 - By BlinkDL. A promising series trained on Neo's dataset that intends to be faster and lighter without compromising on quality. Quality-wise I believe it's on par with GPT-Neo. It's one of the earliest of open-source models to introduce a context length of 4096 or higher.*
**BLOOM** | [560M](https://huggingface.co/bigscience/bloom-560m), [1.1B](https://huggingface.co/bigscience/bloom-1b1), [1.7B](https://huggingface.co/bigscience/bloom-1b7), [3B](https://huggingface.co/bigscience/bloom-3b), [7.1B](https://huggingface.co/bigscience/bloom-7b1),  [176B](https://huggingface.co/bigscience/bloom) | [BigScienceCorpus](https://huggingface.co/bigscience/bloom#training-data) | [BigScience BLOOM RAIL 1.0](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) | *2022 - By BigScience. According to evalutation, this model underperforms compared to the rest. I personally don't recommend it, but I've heard some users get good results with it.*
**GALACTICA** | [125M](https://huggingface.co/facebook/galactica-125m), [1.3B](https://huggingface.co/facebook/galactica-1.3b), [6.7B](https://huggingface.co/facebook/galactica-6.7b), [30B](https://huggingface.co/facebook/galactica-30b), [120B](https://huggingface.co/facebook/galactica-120b) | [Training data](https://huggingface.co/facebook/galactica-120b#training-data) | [CC BY-NC 4.0](https://huggingface.co/facebook/galactica-120b) | *2022 - By Meta AI. I know very little of these models, but I do know they are trained on scientific texts.*
**Pythia Deduped** | [70M](https://huggingface.co/EleutherAI/pythia-70m-deduped), [160M](https://huggingface.co/EleutherAI/pythia-160m-deduped), [410M](https://huggingface.co/EleutherAI/pythia-410m-deduped), [1B](https://huggingface.co/EleutherAI/pythia-1b-deduped), [1.4B](https://huggingface.co/EleutherAI/pythia-1.4b-deduped), [2.8B](https://huggingface.co/EleutherAI/pythia-2.8b-deduped), [6.9B](https://huggingface.co/EleutherAI/pythia-6.9b-deduped), [12B](https://huggingface.co/EleutherAI/pythia-12b-deduped) | [The Pile (deduplicated)](https://huggingface.co/models?dataset=dataset:EleutherAI/the_pile_deduplicated) | [Apache 2.0](https://huggingface.co/EleutherAI/pythia-12b-deduped) | *2022 - By EleutherAI. Could be considered the successor to GPT-Neo. [According to official evaluations it outperforms GPT-Neo and OPT](https://huggingface.co/EleutherAI/pythia-2.8b-deduped#evaluations), likely thanks to the deduplicated dataset.*
**LLaMA** | [7B](https://huggingface.co/Neko-Institute-of-Science/LLaMA-7B-HF), [13B](https://huggingface.co/Neko-Institute-of-Science/LLaMA-13B-HF), [30B](https://huggingface.co/Neko-Institute-of-Science/LLaMA-30B-HF), [65B](https://huggingface.co/Neko-Institute-of-Science/LLaMA-65B-HF) | [Training data](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md#training-dataset) | Unknown (repo says [GPL 3.0](https://twitter.com/ylecun/status/1629189925089296386), but others say non-commercial) | *2023 - By Meta AI. Only officially available to researchers, but has been converted and reuploaded different times. AI evaluation says 13B is on par with GPT-3 175B, and 7B overtakes even Pythia 12B and OPT 66B.*
**Cerebras-GPT** | [111M](https://huggingface.co/cerebras/Cerebras-GPT-111M), [256M](https://huggingface.co/cerebras/Cerebras-GPT-256M), [590M](https://huggingface.co/cerebras/Cerebras-GPT-590M), [1.3B](https://huggingface.co/cerebras/Cerebras-GPT-1.3B), [2.7B](https://huggingface.co/cerebras/Cerebras-GPT-2.7B), [6.7B](https://huggingface.co/cerebras/Cerebras-GPT-6.7B), [13B](https://huggingface.co/cerebras/Cerebras-GPT-13B) | The Pile | [Apache 2.0](https://huggingface.co/cerebras/Cerebras-GPT-13B#model-details) | *2023 - By Cerebras. According to their description their models use The Pile, but unlike Pythia Deduped it isn't deduplicated. Its smaller models seem to underperform according to evaluations, but I haven't tried it myself.*
**RedPajama** | [???](https://www.together.xyz/blog/redpajama) | [Training data](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T) | ??? | *2023 - By Together. Intends to be a reproduction of Meta's LLaMA with an open license. It isn't out yet, so no one can say for sure how it will perform.*
**StableLM** | [Alphas of 3B, 7B currently available; 15B, 30B, 65B, 175B to come later](https://github.com/stability-AI/stableLM/) | "a new experimental dataset built atop The Pile [...] three times larger" | [CC BY-SA 4.0](https://huggingface.co/stabilityai/stablelm-base-alpha-3b) (base models) | *2023 - By StabilityAI. Like RedPajama, I added it to this list for completion, but being a very recent model it's still training and there are no official evaluation results comparing it to the previous in this list.*

### Okay, that's a lot! Which one should I pick?

_Depends on what you want to do with your AI, and what your system requirements are. These are my recommendations:_

- _For generic, flexible models that can be used for many purposes, I recommend base LLaMA for its quality alone, or base [Pythia Deduped](https://huggingface.co/models?search=pythia%20deduped) for its compatibility and ease to find._
- _For novel/co-writer finetunes, I recommend [Janeway](https://huggingface.co/models?sort=downloads&search=janeway) or [Erebus](https://huggingface.co/models?search=erebus) (NSFW)._
- _For AI Dungeon-style text adventure finetunes, I recommend [Nerys](https://huggingface.co/models?search=nerys)._
- _For CharacterAI/Replika-style chatting partner finetunes, I recommend [Pygmalion](https://huggingface.co/models?search=pygmalion) (NSFW)._
- _For ChatGPT-esque assistant finetunes, I currently recommend [Vicuna](https://huggingface.co/models?search=vicuna)._

_For system requirements, [here's a memory usage chart I've made](https://github.com/Crataco/ai-guide/blob/main/charts/memory-usage.md), and [another usage chart by Oobabooga](https://github.com/oobabooga/text-generation-webui/wiki/System-requirements). If you have a gaming computer (with NVIDIA cards being the best supported), you can fit the model entirely on the GPU itself, which will typically take up half as much video RAM as it does regular RAM, and be lightning fast. If you don't have enough VRAM, you can also split it between regular RAM and VRAM, but it will be much slower._

_You can reduce system requirements significantly by looking into the following:_
- _CPU users: llama.cpp, with 4-bit precision, which reduces the RAM usage by a ton - [(Oobabooga Text Gen UI guide)](https://github.com/oobabooga/text-generation-webui/wiki/llama.cpp-models)_
- _CPU and GPU users: RWKV model series, with 8-bit precision - [(Oobabooga Text Gen UI guide)](https://github.com/oobabooga/text-generation-webui/wiki/RWKV-model)_
- _GPU users: 4-bit precision via GPTQ - [(Oobabooga Text Gen UI guide)](https://github.com/oobabooga/text-generation-webui/wiki/GPTQ-models-(4-bit-mode))_
- _GPU users: 8-bit precision on Transformers - (`--load-in-8bit` in Oobabooga's UI)_
