![image](https://user-images.githubusercontent.com/55674863/230696024-98ce9e16-f558-4402-ac43-0e7f960c118c.png)

# Models

### What are models?

A model is your AI. Or to be more specific, its brain.

Models come in all shapes and sizes, and you will see how they differ depending on how big it is, what data was used to train it, and what backend it uses (which most of the time is [Transformers](https://github.com/huggingface/transformers), but recently has been [ggml](https://github.com/ggerganov/ggml)-based backends like [llama.cpp](https://github.com/ggerganov/llama.cpp)).

### How many models are there?

***(If you want a broader list including model evaluation results, [see here](https://docs.google.com/spreadsheets/d/1kT4or6b0Fedd-W_jMwYpb63e1ZR3aePczz3zlbJW-Y4/edit#gid=741531996) (credit goes to [u/randomfoo2 on Reddit](https://old.reddit.com/r/LocalAI/comments/12smsy9/list_of_public_foundational_models_fine_tunes/))***

I note many different model series (including upcoming ones) starting at 2019. These are sorted in chronological order. There may be more (like Facebook/Meta's XGLM), but they aren't notable enough to mention if it's not popular nor are there many finetunes for them.

_Looking for model finetunes? [Check here](https://github.com/Crataco/ai-guide/blob/main/guide/finetunes.md)._

Series | Sizes | Dataset | License | My thoughts
:--|:--:|:--:|:--:|:--:
**GPT-2** | [124M](https://huggingface.co/gpt2), [355M](https://huggingface.co/gpt2-medium), [774M](https://huggingface.co/gpt2-large), [1.5B](https://huggingface.co/gpt2-xl) | [WebText](https://github.com/openai/gpt-2/blob/master/model_card.md#datasets) | [Modified MIT License](https://github.com/openai/gpt-2/blob/master/LICENSE) | *2019 - By OpenAI. Original architecture. GPT-2 kickstarted modern AI text generation, and was most notably used for [the original AI Dungeon 2](https://www.reddit.com/r/rpg/comments/e7ladj/someone_made_an_ai_dungeon_master_that_you_can/). I don't recommend it today due to its small size and small context length (1024 tokens).*
**GPT-Neo** | [125M](https://huggingface.co/EleutherAI/gpt-neo-125M), [350M](https://huggingface.co/xhyi/PT_GPTNEO350_ATG), [1.3B](https://huggingface.co/EleutherAI/gpt-neo-1.3B), [2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B), [6B](https://huggingface.co/EleutherAI/gpt-j-6b), [20B](https://huggingface.co/EleutherAI/gpt-neox-20b) | [The Pile](https://pile.eleuther.ai/) | [MIT](https://huggingface.co/EleutherAI/gpt-neo-2.7B) (125M to 2.7B), [Apache-2.0](https://huggingface.co/EleutherAI/gpt-j-6b) (6B to 20B) | *2021 - By EleutherAI. Original architecture (GPT-Neo for 125M to 2.7B, GPT-J for 6B, GPT-NeoX for 20B). By this time, EleutherAI was a [then-new grassroots collective that responded to GPT-3 with their debut model series, GPT-Neo](https://web.archive.org/web/20210928192557/https://www.eleuther.ai/faq/#general). Outside of LLaMA, NeoX may be the most-supported model architecture.*
**Fairseq** | [125M](https://huggingface.co/KoboldAI/fairseq-dense-125M), [355M](https://huggingface.co/KoboldAI/fairseq-dense-355M), [1.3B](https://huggingface.co/KoboldAI/fairseq-dense-1.3B), [2.7B](https://huggingface.co/KoboldAI/fairseq-dense-2.7B), [6.7B](https://huggingface.co/KoboldAI/fairseq-dense-6.7B), [13B](https://huggingface.co/KoboldAI/fairseq-dense-13B) | [Training Data](https://github.com/facebookresearch/fairseq/blob/main/examples/moe_lm/model_card.md#training-data) | Likely [MIT](https://github.com/facebookresearch/fairseq/blob/main/LICENSE) | *2022 - By Meta AI. Based on the XGLM architecture. Released in a Transformers-compatible format by the KoboldAI community (correct me if I'm wrong). This picked up steam as an alternative to GPT-Neo, and was included alongside Neo with the NovelAI service. Being used in a commercial service makes me believe it uses a commercial license.*
**OPT** | [125M](https://huggingface.co/facebook/opt-125m), [350M](https://huggingface.co/facebook/opt-350m), [1.3B](https://huggingface.co/facebook/opt-1.3b), [2.7B](https://huggingface.co/facebook/opt-2.7b), [6.7B](https://huggingface.co/facebook/opt-6.7b), [13B](https://huggingface.co/facebook/opt-13b), [30B](https://huggingface.co/facebook/opt-30b), [66B](https://huggingface.co/facebook/opt-66b), 175B | [Training Data](https://huggingface.co/facebook/opt-1.3b#training-data) | [Non-commercial OPT-175B license](https://huggingface.co/facebook/opt-1.3b/blob/main/LICENSE.md) | *2022 - By Meta AI. Original architecture. Succeeds Fairseq, their previous series. It slightly outperforms Neo. It comes second to Neo in terms of the sheer quantity of finetunes and community experience. There's a 175B, but it's only available via request.*
**RWKV** | [169M](https://huggingface.co/BlinkDL/rwkv-4-pile-169m), [430M](https://huggingface.co/BlinkDL/rwkv-4-pile-430m), [1.5B](https://huggingface.co/BlinkDL/rwkv-4-pile-1b5), [3B](https://huggingface.co/BlinkDL/rwkv-4-pile-3b), [7B](https://huggingface.co/BlinkDL/rwkv-4-pile-7b), [14B](https://huggingface.co/BlinkDL/rwkv-4-pile-14b) | The Pile | [Apache-2.0](https://huggingface.co/BlinkDL/rwkv-4-pile-14b) | *2022 - By BlinkDL. Original architecture. A promising series trained on Neo's dataset that intends to be faster and lighter without compromising on quality. Quality-wise I believe it's on par with GPT-Neo. It's one of the earliest of open-source models to introduce a context length of 4096 or higher.*
**BLOOM** | [560M](https://huggingface.co/bigscience/bloom-560m), [1.1B](https://huggingface.co/bigscience/bloom-1b1), [1.7B](https://huggingface.co/bigscience/bloom-1b7), [3B](https://huggingface.co/bigscience/bloom-3b), [7.1B](https://huggingface.co/bigscience/bloom-7b1),  [176B](https://huggingface.co/bigscience/bloom) | [BigScienceCorpus](https://huggingface.co/bigscience/bloom#training-data) | [BigScience BLOOM RAIL 1.0](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) | *2022 - By BigScience. Original architecture. According to evalutation, this model underperforms compared to the rest. I personally don't recommend it, but I've heard some users get good results with it.*
**GALACTICA** | [125M](https://huggingface.co/facebook/galactica-125m), [1.3B](https://huggingface.co/facebook/galactica-1.3b), [6.7B](https://huggingface.co/facebook/galactica-6.7b), [30B](https://huggingface.co/facebook/galactica-30b), [120B](https://huggingface.co/facebook/galactica-120b) | [Training data](https://huggingface.co/facebook/galactica-120b#training-data) | [CC BY-NC 4.0](https://huggingface.co/facebook/galactica-120b) | *2022 - By Meta AI. Based on the OPT architecture. I know very little of these models, but I do know they are trained on scientific texts.*
**Pythia Deduped** | [70M](https://huggingface.co/EleutherAI/pythia-70m-deduped), [160M](https://huggingface.co/EleutherAI/pythia-160m-deduped), [410M](https://huggingface.co/EleutherAI/pythia-410m-deduped), [1B](https://huggingface.co/EleutherAI/pythia-1b-deduped), [1.4B](https://huggingface.co/EleutherAI/pythia-1.4b-deduped), [2.8B](https://huggingface.co/EleutherAI/pythia-2.8b-deduped), [6.9B](https://huggingface.co/EleutherAI/pythia-6.9b-deduped), [12B](https://huggingface.co/EleutherAI/pythia-12b-deduped) | [The Pile (deduplicated)](https://huggingface.co/models?dataset=dataset:EleutherAI/the_pile_deduplicated) | [Apache 2.0](https://huggingface.co/EleutherAI/pythia-12b-deduped) | *2022 - By EleutherAI. Based on the GPT-NeoX architecture. Trained on Neo's dataset, but deduplicated, which means that [according to official evaluations it outperforms GPT-Neo and OPT](https://huggingface.co/EleutherAI/pythia-2.8b-deduped#evaluations).*
**LLaMA** | [7B](https://huggingface.co/Neko-Institute-of-Science/LLaMA-7B-HF), [13B](https://huggingface.co/Neko-Institute-of-Science/LLaMA-13B-HF), [30B](https://huggingface.co/Neko-Institute-of-Science/LLaMA-30B-HF), [65B](https://huggingface.co/Neko-Institute-of-Science/LLaMA-65B-HF) | [Training data](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md#training-dataset) | Unknown (repo says [GPL 3.0](https://twitter.com/ylecun/status/1629189925089296386), but others say non-commercial) | *2023 - By Meta AI. Original architecture. Only officially available to researchers, but has been converted and reuploaded everywhere. AI evaluation says 13B is on par with GPT-3 175B, and 7B overtakes even Pythia 12B and OPT 66B.*
**Cerebras-GPT** | [111M](https://huggingface.co/cerebras/Cerebras-GPT-111M), [256M](https://huggingface.co/cerebras/Cerebras-GPT-256M), [590M](https://huggingface.co/cerebras/Cerebras-GPT-590M), [1.3B](https://huggingface.co/cerebras/Cerebras-GPT-1.3B), [2.7B](https://huggingface.co/cerebras/Cerebras-GPT-2.7B), [6.7B](https://huggingface.co/cerebras/Cerebras-GPT-6.7B), [13B](https://huggingface.co/cerebras/Cerebras-GPT-13B) | The Pile | [Apache 2.0](https://huggingface.co/cerebras/Cerebras-GPT-13B#model-details) | *2023 - By Cerebras. [Architecture may be based on "gpt2"](https://huggingface.co/cerebras/Cerebras-GPT-256M/blob/main/config.json). According to their description their models use The Pile, but unlike Pythia Deduped it isn't deduplicated. Its smaller models seem to underperform according to evaluations, but I haven't tried it myself.*
**MPT** | [1B](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b), [7B](https://huggingface.co/mosaicml/mpt-7b) | [RedPajama (1B)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T), [Training data (7B)](https://huggingface.co/mosaicml/mpt-7b#training-data) | Apache 2.0 (base model) | *By MosiacML. Its own architecture. Impressively, according to official benchmarks, ["MPT-7B matches the quality of LLaMA-7B and outperforms other open source 7B - 20B models on standard academic tasks"](https://www.mosaicml.com/blog/mpt-7b), and apparently has a context length up to **checks notes** 84k tokens?! It has yet to come to GGML at the time of writing.*
**OpenLLaMa** | 3B, [7B (preview)](https://huggingface.co/openlm-research) | [RedPajama](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T) | Apache 2.0 | *2023 - By OpenLM Research, using the RedPajama dataset by Together. Based on the LLaMA architecture. A reproduction of LLaMA intended to be released under a truly-open license. Compatible with llama.cpp as a result.*
**StableLM** | [Alphas of 3B, 7B currently available; 15B, 30B, 65B, 175B to come later](https://github.com/stability-AI/stableLM/) | "a new experimental dataset built atop The Pile [...] three times larger" | [CC BY-SA 4.0](https://huggingface.co/stabilityai/stablelm-base-alpha-3b) (base model) | *2023 - By StabilityAI. Based on the GPT-NeoX architecture. Like RedPajama, I added it to this list for completion, but being a very recent model it's still training and there are no official evaluation results comparing it to the previous in this list. Current evaluation results are underwhelming, with 7B apparently performing around the level of Pythia Deduped 410M. Granted, it **should** be early in training.*
**RedPajama-INCITE** | Early releases of [3B](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1) and [7B](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1) | [RedPajama](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T) | Apache-2.0 | *2023 - By Together, following the namesake of their released dataset. Their models are based on the GPT-NeoX architecture and are said to outperform Pythia [according to their blog post](https://www.together.xyz/blog/redpajama-models-v1).*

### Which one should I choose?

Depends on what you want to do with your AI, and what your system requirements are. These are my recommendations:

- For generic, flexible models that can be used for many purposes, I recommend base LLaMA and OpenLLaMA. Alternatively, [Pythia Deduped](https://huggingface.co/models?search=pythia%20deduped) has a wide range of model sizes, but is undertrained by today's standards.
- For novel/co-writer finetunes, I recommend [Janeway](https://huggingface.co/models?sort=downloads&search=janeway) or [Erebus](https://huggingface.co/models?search=erebus) (NSFW), though I assume they're not the best-in-class today as they predate LLaMA.
- For AI Dungeon-style text adventure finetunes, I recommend [Nerys](https://huggingface.co/models?search=nerys), but [gpt4-x-alpaca](https://huggingface.co/eachadea/ggml-gpt4-x-alpaca-13b-native-4bit) (with a "This is a text adventure. User input will be preceded by the > symbol." prompt) worked very well for me, maybe even better than Nerys.
- For CharacterAI/Replika-style chatting partner finetunes, I recommend [Pygmalion](https://huggingface.co/models?search=pygmalion) (NSFW), but base LLaMA has also been said to work well. Once again, [gpt4-x-alpaca](https://huggingface.co/eachadea/ggml-gpt4-x-alpaca-13b-native-4bit) is great, but it takes a bit of effort to pick up on NSFW language.
- For ChatGPT-esque assistant finetunes, I currently recommend [Vicuna](https://huggingface.co/models?search=vicuna). There have been similar/derivative projects such as [WizardLM 7B](https://github.com/nlpxucan/WizardLM) and [WizardVicuna 13B](https://github.com/melodysdreamj/WizardVicunaLM) said to outperform Vicuna, but I've yet to test them extensively.

For system requirements, [here's a memory usage chart I've made](https://github.com/Crataco/ai-guide/blob/main/charts/memory-usage.md), and [another usage chart by Oobabooga](https://github.com/oobabooga/text-generation-webui/wiki/System-requirements). [TheBloke's model reuploads also explain how much RAM you expect to require with them](https://huggingface.co/TheBloke/GPT4All-13B-snoozy-GGML). If you have a gaming computer (with NVIDIA cards having the best support), you can fit the model entirely on the GPU itself, which will typically take up half as much video RAM as it does regular RAM, and be lightning fast. If you don't have enough VRAM, you can also split it between regular RAM and VRAM, but it will be much slower.

You can reduce system requirements significantly by looking into the following options:
- **CPU users:** ggml-based projects, like [koboldcpp](https://github.com/LostRuins/koboldcpp) (backwards compatible with all llama.cpp models). It supports 4-bit, 5-bit, and 8-bit precision, which will drastically reduce RAM usage with negligible quality loss and make such models easier to run on the average PC. - [(Oobabooga Text Gen UI guide)](https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md)
- **CPU and GPU users:** RWKV model series, with 8-bit precision - [(Oobabooga Text Gen UI guide)](https://github.com/oobabooga/text-generation-webui/blob/main/docs/RWKV-model.md)
- **GPU users:** 4-bit precision via GPTQ - [(Oobabooga Text Gen UI guide)](https://github.com/oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md)
- **GPU users:** 8-bit precision on Transformers - (`--load-in-8bit` in Oobabooga's UI)
